{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "\n",
    "from deeppavlov.core.commands.train import read_data_by_config, train_evaluate_model_from_config\n",
    "from deeppavlov.core.commands.infer import interact_model, build_model_from_config\n",
    "from deeppavlov.core.commands.utils import expand_path\n",
    "from deeppavlov.core.common.params import from_params\n",
    "from deeppavlov.core.common.errors import ConfigError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read unlabelled data for label propagation\n",
    "def read_unlabelled_data(UNLABELLED_DATA_PATH):\n",
    "    with open(UNLABELLED_DATA_PATH, \"r\") as f:\n",
    "        unlabelled_data = f.read().splitlines()\n",
    "    return unlabelled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_pl_config(CONFIG_PATH):\n",
    "    config_path_pl = Path(CONFIG_PATH).parent / Path(Path(CONFIG_PATH).stem + \"_pl.json\")\n",
    "\n",
    "    with open(CONFIG_PATH, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    config_pl = deepcopy(config)\n",
    "    config_pl[\"dataset_reader\"][\"train\"] = Path(config_pl[\"dataset_reader\"].get(\"train\", \"train.csv\")).stem + \"_pl.csv\"\n",
    "    \n",
    "    with open(config_path_pl, \"w\") as f:\n",
    "        json.dump(config_pl, f, indent=2)\n",
    "    \n",
    "    return config, config_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_extended_data(config, samples, labels, new_config = None):\n",
    "    train_data = read_data_by_config(deepcopy(config))\n",
    "    \n",
    "    for i in range(len(samples)):\n",
    "        train_data[\"train\"].append((samples[i], labels[i]))\n",
    "    df = pd.DataFrame(train_data[\"train\"], \n",
    "                      columns=[config[\"dataset_reader\"][\"x\"], \n",
    "                               config[\"dataset_reader\"][\"y\"]])\n",
    "    df[config[\"dataset_reader\"][\"y\"]] = df[config[\"dataset_reader\"][\"y\"]].apply(\n",
    "        lambda x: config[\"dataset_reader\"].get(\"class_sep\", \",\").join(x))\n",
    "    \n",
    "    if new_config is not None:\n",
    "        config = new_config\n",
    "    file = expand_path(Path(config[\"dataset_reader\"][\"data_path\"]) / \n",
    "                       Path(config[\"dataset_reader\"][\"train\"]))\n",
    "\n",
    "    if config[\"dataset_reader\"].get(\"format\", \"csv\") == \"csv\":\n",
    "        keys = ('sep', 'header', 'names')\n",
    "        df.to_csv(file, \n",
    "                  index=False,\n",
    "                  sep=config[\"dataset_reader\"].get(\"sep\", \",\")\n",
    "                 )\n",
    "    elif config[\"dataset_reader\"].get(\"format\", \"csv\") == \"json\":\n",
    "        keys = ('orient', 'lines')\n",
    "        df.to_json(file, \n",
    "                  index=False,\n",
    "                  orient=config[\"dataset_reader\"].get(\"orient\", None),\n",
    "                  lines=config[\"dataset_reader\"].get(\"lines\", False)\n",
    "                  )\n",
    "    else:\n",
    "        raise ConfigError(\"Can not work with current data format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually given parameters for pseudo-labeling\n",
    "\n",
    "# path to config file\n",
    "CONFIG_PATH = \"../deeppavlov/configs/classifiers/yahoo_answers_L31_fulltext.json\"\n",
    "# path to file with unlabelled data\n",
    "UNLABELLED_DATA_PATH = \"../download/YahooAnswers/yahoo_answers_data/question_L6.txt\"\n",
    "# number of samples that are going to be labelled during one iteration of label propagation\n",
    "ONE_ITERATION_PORTION = 2000\n",
    "# number of iterations\n",
    "N_ITERATIONS = 10\n",
    "CLASSES_VOCAB_ID_IN_PIPE = 0\n",
    "CONFIDENT_PROBA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read unlabelled dataset\n",
    "unlabelled_data = read_unlabelled_data(UNLABELLED_DATA_PATH)\n",
    "# read config, compose new one, save it\n",
    "config, config_pl = make_pl_config(CONFIG_PATH)\n",
    "# save initial dataset as extended\n",
    "save_extended_data(deepcopy(config), [], [], new_config=deepcopy(config_pl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-08 12:38:43.785 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 89: [saving vocabulary to /home/dilyara/Documents/GitHub/DeepPavlov/download/YahooAnswers/models/model_v8/yahoo_answers_classes.dict]\n",
      "[nltk_data] Downloading package punkt to /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "Using TensorFlow backend.\n",
      "2018-11-08 12:38:44.839 INFO in 'tensorflow'['tf_logging'] at line 159: Using /tmp/tfhub_modules to cache modules.\n",
      "2018-11-08 12:38:45.309 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/aggregation/scaling:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with aggregation/scaling\n",
      "2018-11-08 12:38:45.312 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/aggregation/weights:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with aggregation/weights\n",
      "2018-11-08 12:38:45.315 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN/W_cnn_0:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_0\n",
      "2018-11-08 12:38:45.319 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN/W_cnn_1:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_1\n",
      "2018-11-08 12:38:45.323 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN/W_cnn_2:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_2\n",
      "2018-11-08 12:38:45.327 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN/W_cnn_3:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_3\n",
      "2018-11-08 12:38:45.330 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN/W_cnn_4:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_4\n",
      "2018-11-08 12:38:45.335 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN/W_cnn_5:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_5\n",
      "2018-11-08 12:38:45.338 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN/W_cnn_6:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_6\n",
      "2018-11-08 12:38:45.341 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN/b_cnn_0:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_0\n",
      "2018-11-08 12:38:45.345 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN/b_cnn_1:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_1\n",
      "2018-11-08 12:38:45.348 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN/b_cnn_2:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_2\n",
      "2018-11-08 12:38:45.353 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN/b_cnn_3:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_3\n",
      "2018-11-08 12:38:45.356 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN/b_cnn_4:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_4\n",
      "2018-11-08 12:38:45.359 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN/b_cnn_5:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_5\n",
      "2018-11-08 12:38:45.362 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN/b_cnn_6:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_6\n",
      "2018-11-08 12:38:45.364 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN_high_0/W_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/W_carry\n",
      "2018-11-08 12:38:45.369 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN_high_0/W_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/W_transform\n",
      "2018-11-08 12:38:45.372 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN_high_0/b_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/b_carry\n",
      "2018-11-08 12:38:45.375 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN_high_0/b_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/b_transform\n",
      "2018-11-08 12:38:45.380 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN_high_1/W_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/W_carry\n",
      "2018-11-08 12:38:45.383 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN_high_1/W_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/W_transform\n",
      "2018-11-08 12:38:45.386 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN_high_1/b_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/b_carry\n",
      "2018-11-08 12:38:45.390 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN_high_1/b_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/b_transform\n",
      "2018-11-08 12:38:45.395 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN_proj/W_proj:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_proj/W_proj\n",
      "2018-11-08 12:38:45.398 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/CNN_proj/b_proj:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_proj/b_proj\n",
      "2018-11-08 12:38:45.401 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias\n",
      "2018-11-08 12:38:45.406 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel\n",
      "2018-11-08 12:38:45.409 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-08 12:38:45.413 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias\n",
      "2018-11-08 12:38:45.416 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel\n",
      "2018-11-08 12:38:45.418 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel\n",
      "2018-11-08 12:38:45.421 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias\n",
      "2018-11-08 12:38:45.423 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel\n",
      "2018-11-08 12:38:45.426 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel\n",
      "2018-11-08 12:38:45.429 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias\n",
      "2018-11-08 12:38:45.432 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel\n",
      "2018-11-08 12:38:45.437 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel\n",
      "2018-11-08 12:38:45.439 DEBUG in 'tensorflow'['tf_logging'] at line 100: Initialize variable module/bilm/char_embed:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/char_embed\n",
      "2018-11-08 12:38:46.100 INFO in 'tensorflow'['tf_logging'] at line 115: Saver not created because there are no variables in the graph to restore\n",
      "2018-11-08 12:38:46.295 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 261: [initializing `KerasClassificationModel` from scratch as gru_with_masking_model]\n",
      "2018-11-08 12:38:46.811 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 135: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100, 1024)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100, 1024)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 100, 512), ( 1967616     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 512)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 512)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1536)         0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][2]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1536)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          153700      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 100)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            202         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 2)            0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,121,518\n",
      "Trainable params: 2,121,518\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "2018-11-08 12:39:07.753 INFO in 'deeppavlov.core.commands.train'['train'] at line 360: New best roc_auc of 0.4329\n",
      "2018-11-08 12:39:07.753 INFO in 'deeppavlov.core.commands.train'['train'] at line 362: Saving model\n",
      "2018-11-08 12:39:07.754 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 375: [saving model to /home/dilyara/Documents/GitHub/DeepPavlov/download/YahooAnswers/models/model_v8/model_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 403, \"metrics\": {\"roc_auc\": 0.4329, \"sets_accuracy\": 0.4367, \"f1_macro\": 0.4072}, \"time_spent\": \"0:00:21\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 5}}\n",
      "{\"train\": {\"epochs_done\": 1, \"batches_seen\": 4, \"train_examples_seen\": 3613, \"metrics\": {\"roc_auc\": 0.5411, \"sets_accuracy\": 0.5076, \"f1_macro\": 0.5075}, \"time_spent\": \"0:06:43\", \"loss\": 2.1064522862434387}}\n"
     ]
    }
   ],
   "source": [
    "available_unlabelled_ids = np.arange(len(unlabelled_data))\n",
    "\n",
    "for i in range(N_ITERATIONS):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    \n",
    "    ids_to_label = available_unlabelled_ids[\n",
    "        np.random.randint(low=0, \n",
    "                          high=len(available_unlabelled_ids), \n",
    "                          size=ONE_ITERATION_PORTION)]\n",
    "    available_unlabelled_ids = np.delete(available_unlabelled_ids, ids_to_label)\n",
    "    train_evaluate_model_from_config(config_pl)\n",
    "    model = build_model_from_config(config_pl)\n",
    "    classes = np.array(list(from_params(config_pl[\"chainer\"][\"pipe\"][CLASSES_VOCAB_ID_IN_PIPE]).keys()))\n",
    "\n",
    "    for j, sample_id in enumerate(ids_to_label):\n",
    "        prediction = model([unlabelled_data[sample_id]])[0]\n",
    "        if len(np.where(np.array(prediction) > CONFIDENT_PROBA)):\n",
    "            samples.append(unlabelled_data[sample_id])\n",
    "            labels.append(classes[np.where(np.array(prediction) > CONFIDENT_PROBA)])\n",
    "    \n",
    "    print(\"Iteration {}: add {} samples to train dataset\".format(i, len(samples)))\n",
    "    save_extended_data(config_pl, samples, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep36",
   "language": "python",
   "name": "deep36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
